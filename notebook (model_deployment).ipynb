{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c04de9",
   "metadata": {},
   "source": [
    "## PyTorch model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98f9136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53286e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with regular imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Try to get torchinfo, install it if it doesn't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
    "try:\n",
    "    from going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular .\n",
    "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e75850",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls going_modular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf41abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1653a996",
   "metadata": {},
   "source": [
    "### Getting data\n",
    "\n",
    "Using Pizza, steak, sushi 20% dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47700bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba198a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directory paths to train and test images\n",
    "from pathlib import Path\n",
    "\n",
    "train_dir = Path(\"data/pizza_steak_sushi_20/train\")\n",
    "test_dir = Path(\"data/pizza_steak_sushi_20/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438330c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f33e42",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "1. What is my most ideal machine learning model deployment scenario?\n",
    "2. Where is my model going to go?\n",
    "3. How is my model going to function?\n",
    "\n",
    "**Model should :**\n",
    "1. Perform well: 95%+ accuracy or even more depending on the scenario\n",
    "2. Fast: as close to real-time (or faster) as possible (30FPS+ or 30ms latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca8acb",
   "metadata": {},
   "source": [
    "### Creating an EffNetB2 feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d98623",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# 1. Setup pretrained EffNetB2 weights\n",
    "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "\n",
    "# 2. Get EffNetB2 transforms\n",
    "effnetb2_transforms = effnetb2_weights.transforms()\n",
    "\n",
    "# 3. Setup pretrained model\n",
    "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n",
    "\n",
    "# 4. Freeze the base layers in the model (this will freeze all layers to begin with)\n",
    "for param in effnetb2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out EffNetB2 classifier head\n",
    "effnetb2.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96461a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Update the classifier head\n",
    "effnetb2.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True), # keep dropout layer same\n",
    "    nn.Linear(in_features=1408, # keep in_features same \n",
    "              out_features=3)) # change out_features to suit our number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f40aa54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print EffNetB2 model summary (uncomment for full output) \n",
    "summary(effnetb2, \n",
    "        input_size=(1, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fda414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup DataLoaders\n",
    "from going_modular import data_setup\n",
    "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                 test_dir=test_dir,\n",
    "                                                                                                 train_transform=effnetb2_transforms,\n",
    "                                                                                                 test_transform=effnetb2_transforms,\n",
    "                                                                                                 batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f91fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import engine\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
    "                             lr=1e-3)\n",
    "# Setup loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set seeds for reproducibility and train the model\n",
    "set_seeds()\n",
    "effnetb2_results = engine.train(model=effnetb2,\n",
    "                                train_dataloader=train_dataloader_effnetb2,\n",
    "                                test_dataloader=test_dataloader_effnetb2,\n",
    "                                epochs=20,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn,\n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67baae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(effnetb2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23425ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import utils\n",
    "\n",
    "# Saving EffNetB2 feature extractor\n",
    "utils.save_model_state_dict(model = effnetb2,\n",
    "                           target_dir = \"models\",\n",
    "                           model_name = \"state_dict__effnetb2_trained_on_pizza_steak_sushi_20%.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f4578e",
   "metadata": {},
   "source": [
    "#### Inspecting the size of our EffNetB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a89c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the model size in bytes and convert to megabytes\n",
    "pretrained_effnetb2_model_size = Path(\"models/state_dict__effnetb2_trained_on_pizza_steak_sushi_20%.pth\").stat().st_size/(1024*1024)\n",
    "print(f\"Pretrained effnetb2 feature size is {round(pretrained_effnetb2_model_size,2)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92675233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of parameters in EffNetB2\n",
    "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
    "effnetb2_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create a dictionary with EffNetB2 statistics\n",
    "effnetb2_stats = {\"test loss\": effnetb2_results[\"test_loss\"][-1],\n",
    "                \"test acc\": effnetb2_results[\"test_acc\"][-1],\n",
    "                 \"number of parameters\": effnetb2_total_params,\n",
    "                 \"model size (MB)\": pretrained_effnetb2_model_size}\n",
    "effnetb2_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fccc46",
   "metadata": {},
   "source": [
    "### Creating a ViT feature extractor (base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Get pretrained weights for ViT-Base\n",
    "vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # 'DEFAULT' = best available\n",
    "\n",
    "# Getting automatic transfroms from pretrained ViT weights\n",
    "vit_transforms = vit_weights.transforms()\n",
    "\n",
    "# Setup a ViT model instance with pretrained weights\n",
    "vit = torchvision.models.vit_b_16(weights = vit_weights).to(device)\n",
    "\n",
    "# Freeze the base parameters\n",
    "for parameter in vit.parameters():\n",
    "    parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47cf0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup DataLoaders\n",
    "from going_modular import data_setup\n",
    "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                     test_dir=test_dir,\n",
    "                                                                                     train_transform=vit_transforms,\n",
    "                                                                                     test_transform=vit_transforms,\n",
    "                                                                                     batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacaa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the classifier head\n",
    "vit.heads = nn.Linear(in_features = 768,\n",
    "                                out_features = len(class_names)).to(device)\n",
    "vit.heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary using torchinfo.summary\n",
    "summary(model = vit,\n",
    "       input_size = (1, 3, 224, 224), # (batch_size, number_of_patches, embedding_dimension)\n",
    "       col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "       col_width = 20,\n",
    "       row_settings = [\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d89c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import engine\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
    "                             lr=1e-3)\n",
    "# Setup loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set seeds for reproducibility and train the model\n",
    "set_seeds()\n",
    "vit_results = engine.train(model=vit,\n",
    "                                train_dataloader=train_dataloader_vit,\n",
    "                                test_dataloader=test_dataloader_vit,\n",
    "                                epochs=20,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn,\n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1124a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    " \n",
    "plot_loss_curves(vit_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19409a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving vit feature extractor\n",
    "\n",
    "from going_modular import utils\n",
    "\n",
    "utils.save_model_state_dict(model = vit,\n",
    "                           target_dir = \"models\",\n",
    "                           model_name = \"state_dict__ViT_trained_on_pizza_steak_sushi_20%.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90ed955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the model size in bytes and convert to megabytes\n",
    "pretrained_vit_model_size = Path(\"models/state_dict__vit_trained_on_pizza_steak_sushi_20%.pth\").stat().st_size/(1024*1024)\n",
    "print(f\"Pretrained vit feature size is {round(pretrained_vit_model_size,2)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of parameters in EffNetB2\n",
    "vit_total_params = sum(torch.numel(param) for param in vit.parameters())\n",
    "vit_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create a dictionary with EffNetB2 statistics\n",
    "vit_stats = {\"test loss\": vit_results[\"test_loss\"][-1],\n",
    "            \"test acc\": vit_results[\"test_acc\"][-1],\n",
    "            \"number of parameters\": vit_total_params,\n",
    "            \"model size (MB)\": pretrained_vit_model_size}\n",
    "vit_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24342dd1",
   "metadata": {},
   "source": [
    "### Making predictions with our trained models and timing them\n",
    "\n",
    "* Both the model perform well (95% +)\n",
    "\n",
    "**Testing models:**\n",
    "1. Loop through test images\n",
    "2. Time how long each model takes to make a prediction on the image\n",
    "\n",
    "We need ($30FPS +$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get all test data paths\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
    "test_data_paths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d97f2f",
   "metadata": {},
   "source": [
    "### Creating a function to make predictions accross the test dataset\n",
    "\n",
    "Steps to create `pred_and_store()`:\n",
    "    \n",
    "   1. Create a function that takes a list of patches and a trained PyTorch and a series of transforms a list of target class names and a target device.\n",
    "   2. Create an empty list (can return a full list of all predictions later).\n",
    "   3. Loop through the target input paths (the rest of the steps will take place inside the loop).\n",
    "   4. Create an empty dictionary for each sample (prediction statistics will go in here).\n",
    "   5. Get the sample path and ground truth class from the filepath.\n",
    "   6. Start the prediction timer.\n",
    "   7. Open the image using `PIL.Image.open(path)`.\n",
    "   8. Transform the image to be usable with a given model.\n",
    "   9. Prepare the model for inference by sending to the target device and turning on `eval()` mode.\n",
    "   10. Turn on `torch.inference_mode()` and pass the target transformed image to the model and perform forward pass + calculate pred prob + pred class.\n",
    "   11. Add the pred prob + pred class to empty dictionary from step 4.\n",
    "   12. End the prediction timer started in step 6 and add the time to the prediction dictionary.\n",
    "   13. See if the predicted class matches the ground truth class.\n",
    "   14. Append the updated prediction dictionary to the empty list of predictions we create in step 2.\n",
    "   15. Return the list of prediction dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25dd9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "# 1. Create a function that takes a list of patches and a trained PyTorch and a series of transforms a list of target class names and a target device.\n",
    "def pred_and_store(paths: List[pathlib.Path],\n",
    "                  model: torch.nn.Module,\n",
    "                  transform: torchvision.transforms,\n",
    "                  class_names: List[str],\n",
    "                  device: str = \"cuda\" if torch.cuda.is_available else \"cpu\") -> List[Dict]:\n",
    "    \n",
    "    # 2. Create an empty list (can return a full list of all predictions later)\n",
    "    pred_list = []\n",
    "    \n",
    "    # 3. Loop through the target input paths (the rest of the steps will take place inside the loop)\n",
    "    for path in tqdm(paths):\n",
    "    \n",
    "        # 4. Create an empty dictionary for each sample (prediction statistics will go in here)\n",
    "        pred_dict = {}\n",
    "        \n",
    "        # 5. Get the sample path and ground truth class from the filepath\n",
    "        pred_dict[\"image_path\"] = path\n",
    "        class_name = path.parent.stem\n",
    "        pred_dict[\"class_name\"] = class_name\n",
    "        \n",
    "        # 6. Start the prediction timer\n",
    "        start_time = timer()\n",
    "        \n",
    "        # 7. Open the image using `PIL.Image.open(path)`\n",
    "        img = Image.open(path)\n",
    "        \n",
    "        # 8. Transform the image to be usable with a given model\n",
    "        transformed_image = transform(img).unsqueeze(0).to(device)\n",
    "        \n",
    "        # 9. Prepare the model for inference by sending to the target device and turning on `eval()` mode\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # 10. Turn on `torch.inference_mode()` and pass the target transformed image to the model and perform forward pass + calculate pred prob + pred class\n",
    "        with torch.inference_mode():\n",
    "            pred_logit = model(transformed_image)\n",
    "            pred_prob = torch.softmax(pred_logit, dim = 1)\n",
    "            pred_label = torch.argmax(pred_prob, dim = 1)\n",
    "            pred_class = class_names[pred_label.cpu()] # beacause class_names stay in cpu [all python variables stay in CPU]\n",
    "            \n",
    "            # 11. Add the pred prob + pred class to empty dictionary from step 4\n",
    "            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
    "            pred_dict[\"pred_class\"] = pred_class\n",
    "            \n",
    "            # 12. End the prediction timer started in step 6 and add the time to the prediction dictionary\n",
    "            end_time = timer()\n",
    "            pred_dict[\"time_for_pred\"] = round(end_time - start_time, 4)\n",
    "            \n",
    "        # 13. See if the predicted class matches the ground truth class\n",
    "        pred_dict[\"correct\"] = class_name == pred_class\n",
    "\n",
    "        # 14. Append the updated prediction dictionary to the empty list of predictions we create in step 2\n",
    "        pred_list.append(pred_dict)\n",
    "\n",
    "    # 15. Return the list of prediction dictionaries\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18e2e7",
   "metadata": {},
   "source": [
    "#### Making and timing predictions with EffNetB2\n",
    "\n",
    "Images should be passed through appropriate transformers (e.g ViT with `vit_transforms`)\n",
    "\n",
    "Device should be set correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2_test_pred_dicts = pred_and_store(paths = test_data_paths,\n",
    "                                         model = effnetb2,\n",
    "                                         transform = effnetb2_transforms,\n",
    "                                         class_names = class_names,\n",
    "                                         device = 'cpu')\n",
    "effnetb2_test_pred_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ddd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the test_pred_dicts into a DataFrame\n",
    "import pandas as pd\n",
    "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\n",
    "effnetb2_test_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b58fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of correct predictions\n",
    "effnetb2_test_pred_df.correct.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average time per prediction\n",
    "effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\n",
    "print(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6164a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\n",
    "effnetb2_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f5ea1a",
   "metadata": {},
   "source": [
    "### Vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af456a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_test_pred_dicts = pred_and_store(paths = test_data_paths,\n",
    "                                         model = vit,\n",
    "                                         transform = vit_transforms,\n",
    "                                         class_names = class_names,\n",
    "                                         device = 'cpu')\n",
    "vit_test_pred_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739494d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the test_pred_dicts into a DataFrame\n",
    "import pandas as pd\n",
    "vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\n",
    "vit_test_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of correct predictions\n",
    "vit_test_pred_df.correct.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd774932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average time per prediction\n",
    "vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\n",
    "print(f\"ViT average time per prediction: {vit_average_time_per_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9815488b",
   "metadata": {},
   "source": [
    "**Note:** Prediction times will vary(much like training time) depending on the hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\n",
    "vit_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50337515",
   "metadata": {},
   "source": [
    "## Comparing model results, prediction times and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a875b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn stat dictionaries into DataFrames\n",
    "df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
    "\n",
    "# Add column for model names\n",
    "df[\"model\"] = [\"EffNetB2\", \"ViT\"]\n",
    "\n",
    "# Convert accuracy to percentage\n",
    "df[\"test acc\"] = round(df[\"test acc\"] * 100, 2)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be7ee7",
   "metadata": {},
   "source": [
    "#### Which model is better?\n",
    "\n",
    "* `test_loss` (lower is better) - $ViT$\n",
    "* `test_acc` (higher is better) - $ViT$\n",
    "* `number_of_parameters` (generally lower is better) - $EffNetB2$ (if a model has more parameters, it generally takes longer to compute)\n",
    "     * sometimes models with higher parameters can still perform fast\n",
    "* `model_size (MB)` - $EffNetB2$ (for our use case of deploying to a mobile device, generally lowe is better)\n",
    "* `time_per_pred_cpu` - $EffNetB2$ (lower is better, will be highly dependent on the hardware we're running on)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a667db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ViT to EffNetB2 across different characteristics\n",
    "pd.DataFrame(data = df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"],\n",
    "             columns = [\"ViT to EffNetB2 ratios\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cba4248",
   "metadata": {},
   "source": [
    "### Visualizing the speed vs. performance tradeoff\n",
    "\n",
    "steps:\n",
    "1. Create a scatter plot from the comparision DataFrame to compare EffNetB2 and ViT across test accuracy and prediction time.\n",
    "2. Add titles and labels to make our plot look nice.\n",
    "3. Annotate the samples on the scatter plot so we know what's going on\n",
    "4. Create a legend based on the model sizes `(model_size (MB)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a plot from model comparision DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "scatter = ax.scatter(data = df,\n",
    "                    x = \"time_per_pred_cpu\",\n",
    "                    y = \"test acc\",\n",
    "                    c = [\"blue\", \"orange\"],\n",
    "                    s = \"model size (MB)\")\n",
    "\n",
    "# 2. Add titles and labels\n",
    "ax.set_title(\"FoodVision mini Inference Speed vs Performance\", fontsize = 18)\n",
    "ax.set_xlabel(\"Prediction time per image (seconds)\", fontsize = 14)\n",
    "ax.set_ylabel(\"Test accuracy (%)\", fontsize = 14)\n",
    "ax.tick_params(axis = \"both\", labelsize = 12)\n",
    "ax.grid(True)\n",
    "\n",
    "# 3. Annotate the samples on the scatter plot\n",
    "for index, row in df.iterrows():\n",
    "    ax.annotate(text = row[\"model\"],\n",
    "               xy = (row[\"time_per_pred_cpu\"], row[\"test acc\"]+0.1))\n",
    "\n",
    "# 4. Create a legend based on model sizes\n",
    "handles, labels = scatter.legend_elements(prop = \"sizes\", alpha = .4)\n",
    "model_size_legend = ax.legend(handles,\n",
    "                             labels,\n",
    "                             loc = \"lower right\",\n",
    "                             title = \"Model size (MB)\",\n",
    "                             fontsize = 15)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"foodclassifier-inference-speed-vs-performance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2049e646",
   "metadata": {},
   "source": [
    "#### Top left is the most ideal position for a model to be the best one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4df490",
   "metadata": {},
   "source": [
    "# Bringing model into life by creating Gradio demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31354599",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Import Gradio\n",
    "try:\n",
    "    import gradio as gr\n",
    "except:\n",
    "    !pip install gradio\n",
    "    import gradio as gr\n",
    "    \n",
    "print(f\"Gradio version: {gr.__version__}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fdf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Put our model on the CPU\n",
    "effnetb2 = effnetb2.to(\"cpu\")\n",
    "\n",
    "next(iter(effnetb2.parameters())).device\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2f86c",
   "metadata": {},
   "source": [
    "#### Creating a function called `predict()` \n",
    "\n",
    "`input (images of food) -> Ml model (effnetb2) -> outputs (food class label, prediction time)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd77b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from typing import Tuple, Dict\n",
    "\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    \n",
    "    # Start a timer\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Transform the input image for use with EffNetB2\n",
    "    img = effnetb2_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    # Put the model into eval mode, make prediction\n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass transformed image through the model and turn the prediction logits into probabilities\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim = 1)\n",
    "    \n",
    "    # Create a prediction label and prediction probability dictionary\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    # Calculate pre time\n",
    "    end_time = timer()\n",
    "    pred_time = round(end_time - start_time, 4)\n",
    "    \n",
    "    # Return pred dict and pred time\n",
    "    return pred_labels_and_probs, pred_time\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import random\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "# Get a list of all test image filepaths\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
    "print(f\"Example test data path: { test_data_paths[0]}\")\n",
    "\n",
    "# Randomly select a test image path\n",
    "random_image_path = random.sample(test_data_paths, k = 1)[0]\n",
    "random_image_path\n",
    "\n",
    "# Open the target image\n",
    "image = Image.open(random_image_path)\n",
    "print(f\"[INFO] Predicting on image on path: {random_image_path}\\n\")\n",
    "\n",
    "# Predict on the target image and print out the outputs\n",
    "pred_dict, pred_time = predict(img = image)\n",
    "print(pred_dict)\n",
    "print(pred_time)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb22173",
   "metadata": {},
   "source": [
    "#### Creating a list of example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed3e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of examples inputs to Gradio demo\n",
    "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k = 3)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3241d003",
   "metadata": {},
   "source": [
    "#### Building a Gradio Interface\n",
    "\n",
    "Using `gr.Interface()`\n",
    "\n",
    "\n",
    "\n",
    "`input: image -> transform -> predict with EffNetB2 -> output: pred, pred prob, time\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import gradio as gr\n",
    "\n",
    "# Create title, description and article\n",
    "title = 'FoodIdentifier 🍣🍕🥩'\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images as pizza, sushi or steak\"\n",
    "article = \" anything I want for the description of the description above 🤪\"\n",
    "\n",
    "# Create the Gradio demo\n",
    "demo = gr.Interface(fn = predict, # maps input to output\n",
    "                    inputs = gr.Image(type = 'pil'),\n",
    "                    outputs = [gr.Label(num_top_classes = 3, label = \"Predictions\"),\n",
    "                              gr.Number(label = \"Prediction time (s)\")],\n",
    "                    examples = example_list,\n",
    "                    title = title,\n",
    "                    description = description,\n",
    "                    article = article\n",
    "                   )\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch(debug = False, # print errors locally?\n",
    "           share = True) # generate a publically shareable URL\n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da8606",
   "metadata": {},
   "source": [
    "## Turning food indentifier to a deployable app\n",
    "\n",
    "Our gradio demo are fantastic but they expire within 72 hours\n",
    "\n",
    "##### Hugging Face Spaces\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a demo folder to store food identifier app files\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Create FoodVision demo path\n",
    "foodvision_demo_path = Path(\"demos/foodvision_mini\")\n",
    "\n",
    "# Remove files that might exist and create a new directory\n",
    "if foodvision_demo_path.exists():\n",
    "    shutil.rmtree(foodvision_demo_path)\n",
    "    foodvision_demo_path.mkdir(parents = True,\n",
    "                              exist_ok = True)\n",
    "else:\n",
    "    foodvision_demo_path.mkdir(parents = True,\n",
    "                              exist_ok = True)\n",
    "    \n",
    "!ls demos/foodvision_mini/ # we have no files currently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92f3f2",
   "metadata": {},
   "source": [
    "#### Creating a folder of example images to use with the demo app\n",
    "* Using 3 images from the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180af71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a example directory\n",
    "foodvision_mini_examples_path = foodvision_demo_path/\"examples\"\n",
    "foodvision_mini_examples_path.mkdir(parents = True,\n",
    "                                   exist_ok = True)\n",
    "\n",
    "# Paths for example images\n",
    "food_mini_examples = [Path('data\\sushi_served.png'),\n",
    "                     Path('data\\pizza_on_fire.jpg'),\n",
    "                     Path('data\\steak.jpg')]\n",
    "\n",
    "# Copy the three images to the examples directory\n",
    "for example in food_mini_examples:\n",
    "    destination = foodvision_mini_examples_path / example.name\n",
    "    print(f\"Copying {example} to {destination}\")\n",
    "    shutil.copy2(src = example,\n",
    "                dst = destination)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad6500",
   "metadata": {},
   "source": [
    " Let's verify that we can get a list of list from our `examples/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f646a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get example filepaths in a list of lists\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving trained EffNet model to our Foodvision demos directory\n",
    "\n",
    "# Create a source path for our target model\n",
    "effnet_foodvision_mini_model_path = \"models/state_dict__effnetb2_trained_on_pizza_steak_sushi_20%.pth\"\n",
    "\n",
    "# Create a destination path for target model\n",
    "effnetb2_model_demo_destination = foodvision_demo_path/effnet_foodvision_mini_model_path.split(\"/\")[1]\n",
    "\n",
    "# Moving the model file\n",
    "try:\n",
    "    print(f\"Attempting to move effnetb2 model path to {effnetb2_model_demo_destination}\\n\")\n",
    "    \n",
    "    # Move the model\n",
    "    shutil.move(src = effnet_foodvision_mini_model_path,\n",
    "               dst = effnetb2_model_demo_destination)\n",
    "    print(\"Model moved complete\\n\")\n",
    "\n",
    "# If the model has already been moved, check if it exists\n",
    "except:\n",
    "    print(f\"No model found at {effnet_foodvision_mini_model_path}, perhaps it already moved\\n\")\n",
    "    print(f\"Model exists at {effnetb2_model_demo_destination} : {effnetb2_model_demo_destination.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c7f45",
   "metadata": {},
   "source": [
    "### Turning off EffNetB2 model into a Python script `model.py`\n",
    "\n",
    "We have a saved `.pth` model `state_dict` and want to load it into a model instance.\n",
    "\n",
    "Let's move our `create_effnetb2_model()` function to a script so we can reuse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784fde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_mini/model.py\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def create_effnetb2_model(num_classes:int=3, \n",
    "                          seed:int=42):\n",
    "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head. \n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): EffNetB2 feature extractor model. \n",
    "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
    "    \"\"\"\n",
    "    # Create EffNetB2 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head with random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes),\n",
    "    )\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4bb74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from demos.foodvision_mini import model\n",
    "\n",
    "effnet_model, effnetb2_transforms_import = model.create_effnetb2_model()\n",
    "effnetb2_transforms_import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7357f133",
   "metadata": {},
   "source": [
    "### Turning Foodvision Gradio app into a Python script\n",
    "\n",
    "the `app.py` file will have major parts:\n",
    "1. imports and class names setup\n",
    "2. model and transforms preparation\n",
    "3. predict function `predict()`\n",
    "4. Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de6dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_mini/app.py\n",
    "\n",
    "# 1.\n",
    "import gradio as gr\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from model import create_effnetb2_model\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Setup class names\n",
    "class_names = ['pizza', 'steak', 'sushi']\n",
    "\n",
    "# 2.\n",
    "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes = len(class_names))\n",
    "\n",
    "# Load saved weights\n",
    "effnetb2.load_state_dict(torch.load(f = \"state_dict__effnetb2_trained_on_pizza_steak_sushi_20%.pth\",\n",
    "                                   map_location = torch.device(\"cpu\")))\n",
    "\n",
    "# 3.\n",
    "\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    \n",
    "    # Start a timer\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Transform the input image for use with EffNetB2\n",
    "    img = effnetb2_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    # Put the model into eval mode, make prediction\n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass transformed image through the model and turn the prediction logits into probabilities\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim = 1)\n",
    "    \n",
    "    # Create a prediction label and prediction probability dictionary\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    # Calculate pre time\n",
    "    end_time = timer()\n",
    "    pred_time = round(end_time - start_time, 4)\n",
    "    \n",
    "    # Return pred dict and pred time\n",
    "    return pred_labels_and_probs, pred_time\n",
    "\n",
    "# 4.\n",
    "title = 'FoodIdentifier 🍣🍕🥩'\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images as pizza, sushi or steak\"\n",
    "article = \" anything I want for the description of the description above 🤪\"\n",
    "\n",
    "# Create example list\n",
    "# Get example filepaths in a list of lists\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "# Create the Gradio demo\n",
    "demo = gr.Interface(fn = predict, # maps input to output\n",
    "                    inputs = gr.Image(type = 'pil'),\n",
    "                    outputs = [gr.Label(num_top_classes = 3, label = \"Predictions\"),\n",
    "                              gr.Number(label = \"Prediction time (s)\")],\n",
    "                    examples = example_list,\n",
    "                    title = title,\n",
    "                    description = description,\n",
    "                    article = article\n",
    "                   )\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch(debug = False, # print errors locally?\n",
    "           share = True) # generate a publically shareable URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef95449",
   "metadata": {},
   "source": [
    "#### Creating `requirements.txt`\n",
    "\n",
    "The requirement file will tell hugging face space what software dependencies are required for the app\n",
    "\n",
    "The three main ones are:\n",
    "* torch\n",
    "* torchvision\n",
    "* gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d58c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_mini/requirements.txt\n",
    "\n",
    "torch>1.12.0\n",
    "torchvision>0.13.0\n",
    "gradio>3.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83096d0a",
   "metadata": {},
   "source": [
    "### Running Gradio app locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4f0f34",
   "metadata": {},
   "source": [
    "### Deploying foodvision mini app into HuggingFace\n",
    "Upload files in proper format like done in this notebook with the help of git.\n",
    "\n",
    "### Hugging face to notebook\n",
    "Below code helps.\n",
    "\n",
    "this format must be the same in all except the username and space name\n",
    "https://hf.space/embed/ayusk/food_identifier-pizza-steak-sushi/+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c247d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ipython is a library to help make Python interactive\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Embed FoodVision Mini Gradio demo\n",
    "IFrame(src = \"https://hf.space/embed/ayusk/food_identifier-pizza-steak-sushi/+\", width = 900, height = 750)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f6f46",
   "metadata": {},
   "source": [
    "# Creating FoodVision Big + Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de664d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Food101 model and transforms\n",
    "effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36082db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print EffNetB2 model summary (uncomment for full output) \n",
    "summary(effnetb2_food101, \n",
    "        input_size=(1, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883b10a9",
   "metadata": {},
   "source": [
    "Since we're working with a larget dataset, we may want to introduce some data augmentation techniques:\n",
    "* This is because with larger datasets and larger models, overfitting become more of a problem.\n",
    "* Because we're working with a large number of classes, let's use TrivialAugment as our data augmentation technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce6b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training transforms\n",
    "food101_train_transform = torchvision.transforms.Compose([torchvision.transforms.TrivialAugmentWide(),\n",
    "                                                      effnetb2_transforms])\n",
    "\n",
    "food101_train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data transforms\n",
    "effnetb2_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5eeac5",
   "metadata": {},
   "source": [
    "## Getting food vision big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3aa166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "# Setup data directory\n",
    "from pathlib import Path\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "# Get the training data (-750 images x 101 classes)\n",
    "train_data = datasets.Food101(root = data_dir,\n",
    "                             split = \"train\",\n",
    "                             transform = effnet_b2_foodvision100_train_transforms, # transform training data\n",
    "                             download = True)\n",
    "\n",
    "# Get the testing data (-250 images x 101 classes)\n",
    "test_data = datasets.Food101(root = data_dir,\n",
    "                             split = \"test\",\n",
    "                             transform = effnet_b2_foodvision100_test_transforms, # transform test data\n",
    "                             download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get food101 class name\n",
    "food101_class_names = train_data.classes\n",
    "\n",
    "# View the first 10\n",
    "food101_class_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509441a",
   "metadata": {},
   "source": [
    "### Creating a subset of the food101 dataset for faster experimenting\n",
    "\n",
    "making 20% subset of the dataset (training and test)\n",
    "\n",
    "Our short goal: TO beat the original food101 paper result of 56.40% accuracy on the test dataset \n",
    "\n",
    "Paper source : https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/static/bossard_eccv14_food-101.pdf\n",
    "\n",
    "This was done using RandomForest. We want to beat it using modern deep learning networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdae19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "def split_dataset(dataset: torchvision.datasets,\n",
    "                 split_size:float = 0.2,\n",
    "                 seed: int = 42):\n",
    "    \n",
    "    # Create split lengths based on original dataset length\n",
    "    length_1 = int(len(dataset) * split_size)\n",
    "    length_2 = len(dataset) - length_1 # remaining length\n",
    "    \n",
    "    # Print out info\n",
    "    print(f\"[INFO] Splitting dataset of length {len(dataset)} into splits of size: {length_1} and {length_2}\")\n",
    "    \n",
    "    # Create splits with given random seed\n",
    "    random_split_1, random_split_2 = torch.utils.data.random_split(dataset,\n",
    "                                                                  lengths = [length_1, length_2],\n",
    "                                                                  generator = torch.manual_seed(seed))\n",
    "    \n",
    "    return random_split_1, random_split_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf8ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training 20% split Food101\n",
    "train_data_food101_20_percent, _ = split_dataset(dataset = train_data,\n",
    "                                             split_size = 0.2)\n",
    "\n",
    "# Create testing 20% split food101\n",
    "test_data_food101_20_percent, _ = split_dataset(dataset = test_data,\n",
    "                                               split_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4649485e",
   "metadata": {},
   "source": [
    "### Turning food101 datasets into `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create food101 20 training DataLoader\n",
    "train_dataloader_food101_20_percent = torch.utils.data.DataLoader(dataset = train_data_food101_20_percent,\n",
    "                                                                 batch_size= BATCH_SIZE,\n",
    "                                                                 shuffle = True,\n",
    "                                                                 num_workers = NUM_WORKERS)\n",
    "\n",
    "# Create food101 20% testing DataLoader\n",
    "test_dataloader_food101_20_percent = torch.utils.data.DataLoader(dataset = test_data_food101_20_percent,\n",
    "                                                                 batch_size= BATCH_SIZE,\n",
    "                                                                 shuffle = False,\n",
    "                                                                 num_workers = NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbff77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader_food101_20_percent), len(test_dataloader_food101_20_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d90b1ea",
   "metadata": {},
   "source": [
    "### Training food vision big\n",
    "\n",
    "Things for training\n",
    "* 5 epochs\n",
    "* Optimizer: torch.optim.Adam(lr = 1e-3)\n",
    "* Loss function: torch.nn.CrossEntropyLoss(label_smoothing = 0.1)\n",
    "\n",
    "Label smoothing helps to prevent overfitting (it's a regularization technique).\n",
    "\n",
    "Without label smoothing and 5 classes.\n",
    "`[0.00, 0.00, 0.99, 0.01, 0.00]`\n",
    "\n",
    "With label smoothing and 5 classes.\n",
    "`[0.01, 0.01, 0.96, 0.01, 0.01]` \n",
    "\n",
    "**Label Smoothing (regularization technique)** helps assign atleast some value to other classes which prevents model from being over confidient (over-fitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1148e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import engine\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.Adam(params = effnetb2_food101.parameters(),\n",
    "                            lr = 1e-3)\n",
    "\n",
    "# Loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing = 0.1)\n",
    "\n",
    "# Want to beat the original food101 paper's result of 56.4% accuracy on the test dataset with 20% of the data\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "effnet_food101_results = engine.train(model=effnetb2_food101,\n",
    "                                train_dataloader=train_dataloader_food101_20_percent,\n",
    "                                test_dataloader=test_dataloader_food101_20_percent,\n",
    "                                epochs=5,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn,\n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ce429",
   "metadata": {},
   "source": [
    "### Inspecting loss curves of foodvision Big model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(effnet_food101_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f57bdf",
   "metadata": {},
   "source": [
    "### Save and load FoodVision Big model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b0426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from going_modular import utils\n",
    "\n",
    "# Create a model path\n",
    "effnetb2_food101_model_path = \"state_dict__effnetb2_food101_20_percent.pth\"\n",
    "\n",
    "# Save the FoodVision Big model\n",
    "utils.save_model_state_dict(model = effnetb2_food101,\n",
    "                target_dir = 'models/',\n",
    "                model_name = effnetb2_food101_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7986563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create food101 compatible EffNetB2 instance\n",
    "loaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes = 101)\n",
    "\n",
    "# Load the saved model's state dict()\n",
    "loaded_effnetb2_food101.load_state_dict(torch.load(\"models/state_dict__effnetb2_food101_20_percent.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a10f858",
   "metadata": {},
   "source": [
    "#### Checking foodvision big model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68282dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the model size in bytes then convert to megabytes\n",
    "pretrained_effnetb2_food101_size = Path(\"models\", effnetb2_food101_model_path).stat().st_size // (1024 * 1024)\n",
    "print(f\"effnetb2 foodvision big 20 percent model size : {pretrained_effnetb2_food101_size} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d5746",
   "metadata": {},
   "source": [
    "## Turning foodvision big model into a deployable app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63386bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path to Food101 class names\n",
    "foodvision_big_class_names_path = foodvision_big_path / \"class_names.txt\"\n",
    "foodvision_big_class_names_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f9086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create FoodVision Big demo path\n",
    "foodvision_big_demo_path = Path(\"demos/foodvision_big\")\n",
    "\n",
    "# Make foodvision big demo directory\n",
    "foodvision_big_demo_path.mkdir(parents = True,\n",
    "                              exist_ok = True)\n",
    "\n",
    "# Make foodvision big demo examples directory\n",
    "(foodvision_big_demo_path / \"examples\").mkdir(parents = True,\n",
    "                                             exist_ok = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46eefa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class names\n",
    "food101_class_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71762a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path to Food101 class names\n",
    "foodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"\n",
    "foodvision_big_class_names_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d339cc15",
   "metadata": {},
   "source": [
    "#### Write food101 class names to text file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa04005",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(foodvision_big_class_names_path, \"w\") as f:\n",
    "    print(f\"[INFO] saving food101 class names to {foodvision_big_class_names_path}\")\n",
    "    f.write(\"\\n\".join(food101_class_names)) # new line per class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6862560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open food101 class names file and read each line into a list\n",
    "with open(foodvision_big_class_names_path, \"r\") as f:\n",
    "    food101_class_names_loaded = [food.strip('\\n') for food in f.readlines()]\n",
    "    \n",
    "food101_class_names_loaded[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269aebfb",
   "metadata": {},
   "source": [
    "#### model.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dae0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_big/model.py\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def create_effnetb2_model(num_classes:int=3, \n",
    "                          seed:int=42):\n",
    "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head. \n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): EffNetB2 feature extractor model. \n",
    "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
    "    \"\"\"\n",
    "    # Create EffNetB2 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head with random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes),\n",
    "    )\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63a41fb",
   "metadata": {},
   "source": [
    "#### app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff6203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_big/app.py\n",
    "\n",
    "# 1.\n",
    "import gradio as gr\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from model import create_effnetb2_model\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Setup class names\n",
    "with open(\"class_names.txt\", \"r\") as f:\n",
    "    class_names = [food.strip('\\n') for food in f.readlines()]\n",
    "\n",
    "# 2.\n",
    "effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes = 101)\n",
    "\n",
    "# Load saved weights\n",
    "effnetb2_food101.load_state_dict(torch.load(\"models/state_dict__effnetb2_food101_20_percent.pth\",\n",
    "                                           map_location = torch.device('cpu')))\n",
    "\n",
    "\n",
    "# 3.\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    \n",
    "    # Start a timer\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Transform the input image for use with EffNetB2\n",
    "    img = effnetb2_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    # Put the model into eval mode, make prediction\n",
    "    effnetb2_food101.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass transformed image through the model and turn the prediction logits into probabilities\n",
    "        pred_probs = torch.softmax(effnetb2_food101(img), dim = 1)\n",
    "    \n",
    "    # Create a prediction label and prediction probability dictionary\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    # Calculate pre time\n",
    "    end_time = timer()\n",
    "    pred_time = round(end_time - start_time, 4)\n",
    "    \n",
    "    # Return pred dict and pred time\n",
    "    return pred_labels_and_probs, pred_time\n",
    "\n",
    "# 4.\n",
    "title = 'FoodIdentifier Big (a little) 🍣🍕🥩'\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images as pizza, sushi or steak\"\n",
    "article = \" anything I want for the description of the description above 🤪\"\n",
    "\n",
    "# Create example list\n",
    "# Get example filepaths in a list of lists\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "# Create the Gradio demo\n",
    "demo = gr.Interface(fn = predict, # maps input to output\n",
    "                    inputs = gr.Image(type = 'pil'),\n",
    "                    outputs = [gr.Label(num_top_classes = 5, label = \"Predictions\"),\n",
    "                              gr.Number(label = \"Prediction time (s)\")],\n",
    "                    examples = example_list,\n",
    "                    title = title,\n",
    "                    description = description,\n",
    "                    article = article\n",
    "                   )\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch(debug = False, # print errors locally?\n",
    "           share = True) # generate a publically shareable URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81fd299",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_big/requirements.txt\n",
    "\n",
    "torch>1.12.0\n",
    "torchvision>0.13.0\n",
    "gradio>3.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72183f58",
   "metadata": {},
   "source": [
    "# Food Vision Big (100% data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c527352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def create_effnetb2_model(num_classes:int=3, \n",
    "                          seed:int=42):\n",
    "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head. \n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): EffNetB2 feature extractor model. \n",
    "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
    "    \"\"\"\n",
    "    # Create EffNetB2 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head with random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes),\n",
    "    )\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b66a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnet_b2_foodvision100, effnet_b2_foodvision100_test_transforms = create_effnetb2_model(num_classes = 101,\n",
    "                                                                                   seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe63e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training transforms\n",
    "effnet_b2_foodvision100_train_transforms = torchvision.transforms.Compose([torchvision.transforms.TrivialAugmentWide(),\n",
    "                                                      effnet_b2_foodvision100_test_transforms])\n",
    "\n",
    "effnet_b2_foodvision100_train_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print EffNetB2 model summary (uncomment for full output) \n",
    "summary(effnet_b2_foodvision100, \n",
    "        input_size=(1, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783dcaa",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6629cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "# Setup data directory\n",
    "from pathlib import Path\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "# Get the training data (-750 images x 101 classes)\n",
    "train_data = datasets.Food101(root = data_dir,\n",
    "                             split = \"train\",\n",
    "                             transform = effnet_b2_foodvision100_train_transforms, # transform training data\n",
    "                             download = True)\n",
    "\n",
    "# Get the testing data (-250 images x 101 classes)\n",
    "test_data = datasets.Food101(root = data_dir,\n",
    "                             split = \"test\",\n",
    "                             transform = effnet_b2_foodvision100_test_transforms, # transform test data\n",
    "                             download = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9167b4",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949ec56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create food101 20 training DataLoader\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset = train_data,\n",
    "                                                batch_size= BATCH_SIZE,\n",
    "                                                shuffle = True,\n",
    "                                                num_workers = NUM_WORKERS)\n",
    "\n",
    "# Create food101 20% testing DataLoader\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset = test_data,\n",
    "                                                 batch_size= BATCH_SIZE,\n",
    "                                                 shuffle = False,\n",
    "                                                 num_workers = NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11285525",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80789d36",
   "metadata": {},
   "source": [
    "### Training food vision big\n",
    "\n",
    "Things for training\n",
    "* 5 epochs\n",
    "* Optimizer: torch.optim.Adam(lr = 1e-3)\n",
    "* Loss function: torch.nn.CrossEntropyLoss(label_smoothing = 0.1)\n",
    "\n",
    "Label smoothing helps to prevent overfitting (it's a regularization technique).\n",
    "\n",
    "Without label smoothing and 5 classes.\n",
    "`[0.00, 0.00, 0.99, 0.01, 0.00]`\n",
    "\n",
    "With label smoothing and 5 classes.\n",
    "`[0.01, 0.01, 0.96, 0.01, 0.01]` \n",
    "\n",
    "**Label Smoothing (regularization technique)** helps assign atleast some value to other classes which prevents model from being over confidient (over-fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dfa0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffda415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import engine\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.Adam(params = effnet_b2_foodvision100.parameters(),\n",
    "                            lr = 1e-3)\n",
    "\n",
    "# Loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing = 0.1)\n",
    "\n",
    "# Want to beat the original food101 paper's result of 56.4% accuracy on the test dataset with 100% of the data\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "effnet_food101_full_results = engine.train(model=effnet_b2_foodvision100,\n",
    "                                train_dataloader=train_dataloader,\n",
    "                                test_dataloader=test_dataloader,\n",
    "                                epochs=20,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn,\n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(effnet_food101_full_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f4edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create FoodVision Big demo path\n",
    "foodvision_big_path = Path(\"demos/foodvision_100percent\")\n",
    "\n",
    "# Make foodvision big demo directory\n",
    "foodvision_big_path.mkdir(parents = True,\n",
    "                              exist_ok = True)\n",
    "\n",
    "# Make foodvision big demo examples directory\n",
    "(foodvision_big_path / \"examples\").mkdir(parents = True,\n",
    "                                             exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ba7e8",
   "metadata": {},
   "source": [
    "### Save and load FoodVision Big model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef5ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import utils\n",
    "\n",
    "# Create a model path\n",
    "effnetb2_food101_full_model_path = \"state_dict__effnetb2_food101_100_percent.pth\"\n",
    "\n",
    "# Save the FoodVision Big model\n",
    "utils.save_model_state_dict(model = effnet_b2_foodvision100,\n",
    "                target_dir = 'demos/foodvision_100percent/models/',\n",
    "                model_name = effnetb2_food101_full_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b888e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create food101 compatible EffNetB2 instance\n",
    "loaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes = 101)\n",
    "\n",
    "# Load the saved model's state dict()\n",
    "loaded_effnetb2_food101.load_state_dict(torch.load(\"demos/foodvision_100percent/models/state_dict__effnetb2_food101_100_percent.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704bef4",
   "metadata": {},
   "source": [
    "### Checking foodvision big model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ec3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the model size in bytes then convert to megabytes\n",
    "pretrained_effnetb2_food101_big_size = Path(\"demos/foodvision_100percent/models\", effnetb2_food101_full_model_path).stat().st_size // (1024 * 1024)\n",
    "print(f\"effnetb2 foodvision big full model size : {pretrained_effnetb2_food101_big_size} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad321954",
   "metadata": {},
   "source": [
    "# VIT full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d14b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Get pretrained weights for ViT-Base\n",
    "vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # 'DEFAULT' = best available\n",
    "\n",
    "# Getting automatic transfroms from pretrained ViT weights\n",
    "vit_transforms = vit_weights.transforms()\n",
    "\n",
    "# Setup a ViT model instance with pretrained weights\n",
    "vit = torchvision.models.vit_b_16(weights = vit_weights).to(device)\n",
    "\n",
    "# Freeze the base parameters\n",
    "for parameter in vit.parameters():\n",
    "    parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fce682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training transforms\n",
    "vit_train_transforms = torchvision.transforms.Compose([torchvision.transforms.TrivialAugmentWide(),\n",
    "                                                      vit_transforms])\n",
    "\n",
    "vit_train_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3d9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the classifier head\n",
    "vit.heads = nn.Linear(in_features = 768,\n",
    "                    out_features = 101).to(device)\n",
    "vit.heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50190c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "# Setup data directory\n",
    "from pathlib import Path\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "# Get the training data (-750 images x 101 classes)\n",
    "train_data_vit = datasets.Food101(root = data_dir,\n",
    "                             split = \"train\",\n",
    "                             transform = vit_train_transforms, # transform training data\n",
    "                             download = True)\n",
    "\n",
    "# Get the testing data (-250 images x 101 classes)\n",
    "test_data_vit = datasets.Food101(root = data_dir,\n",
    "                             split = \"test\",\n",
    "                             transform = vit_transforms, # transform test data\n",
    "                             download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43425456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create food101 20 training DataLoader\n",
    "train_dataloader_vit = torch.utils.data.DataLoader(dataset = train_data_vit,\n",
    "                                                                 batch_size= BATCH_SIZE,\n",
    "                                                                 shuffle = True,\n",
    "                                                                 num_workers = NUM_WORKERS)\n",
    "\n",
    "# Create food101 20% testing DataLoader\n",
    "test_dataloader_vit = torch.utils.data.DataLoader(dataset = test_data_vit,\n",
    "                                                                 batch_size= BATCH_SIZE,\n",
    "                                                                 shuffle = False,\n",
    "                                                                 num_workers = NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0341ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary using torchinfo.summary\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(model = vit,\n",
    "       input_size = (1, 3, 224, 224), # (batch_size, number_of_patches, embedding_dimension)\n",
    "       col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "       col_width = 20,\n",
    "       row_settings = [\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f984b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(vit.parameters())).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import engine\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
    "                             lr=1e-3)\n",
    "# Setup loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing = 0.1)\n",
    "\n",
    "# Set seeds for reproducibility and train the model\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "vit_results = engine.train(model=vit,\n",
    "                                train_dataloader=train_dataloader_vit,\n",
    "                                test_dataloader=test_dataloader_vit,\n",
    "                                epochs=7,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn,\n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d0a88f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(vit_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82632fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(vit_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6074c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import utils\n",
    "\n",
    "# Create a model path\n",
    "vit_food101_full_model_path = \"state_dict__vit_food101_100_percent.pth\"\n",
    "\n",
    "# Save the FoodVision Big model\n",
    "utils.save_model_state_dict(model = vit,\n",
    "                target_dir = 'demos/foodvision_100percent/models/',\n",
    "                model_name = vit_food101_full_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857bae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model to load saved state dict()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Get pretrained weights for ViT-Base\n",
    "vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # 'DEFAULT' = best available\n",
    "\n",
    "# Getting automatic transfroms from pretrained ViT weights\n",
    "vit_transforms = vit_weights.transforms()\n",
    "\n",
    "# Setup a ViT model instance with pretrained weights\n",
    "loaded_vit_food101 = torchvision.models.vit_b_16(weights = vit_weights).to(device)\n",
    "\n",
    "# Freeze the base parameters\n",
    "for parameter in loaded_vit_food101.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# Update the classifier head\n",
    "loaded_vit_food101.heads = nn.Linear(in_features = 768,\n",
    "                    out_features = 101).to(device)\n",
    "    \n",
    "    \n",
    "# Load the saved model's state dict()\n",
    "loaded_vit_food101.load_state_dict(torch.load(\"demos/foodvision_100percent/models/state_dict__vit_food101_100_percent.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3efe754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "vit_food101_full_model_path = \"state_dict__vit_food101_100_percent.pth\"\n",
    "\n",
    "\n",
    "# Get the model size in bytes then convert to megabytes\n",
    "vit_food101_big_size = Path(\"demos/foodvision_100percent/models\", vit_food101_full_model_path).stat().st_size // (1024 * 1024)\n",
    "print(f\"ViT foodvision big full model size : {vit_food101_big_size} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc97fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of parameters in ViT\n",
    "vit_total_params = sum(torch.numel(param) for param in loaded_vit_food101.parameters())\n",
    "vit_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a4d78",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create a dictionary with ViT statistics\n",
    "vit_stats = {\"test loss\": vit_results[\"test_loss\"][-1],\n",
    "            \"test acc\": vit_results[\"test_acc\"][-1],\n",
    "            \"number of parameters\": vit_total_params,\n",
    "            \"model size (MB)\": vit_food101_big_size}\n",
    "vit_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363668e4",
   "metadata": {},
   "source": [
    "### Making predictions with our trained models and timing them\n",
    "\n",
    "* Both the model perform well (95% +)\n",
    "\n",
    "**Testing models:**\n",
    "1. Loop through test images\n",
    "2. Time how long each model takes to make a prediction on the image\n",
    "\n",
    "We need ($30FPS +$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d363a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader_.sampler.data_source.dataset.imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a200aa",
   "metadata": {},
   "source": [
    "# Turning foodvision big model into a deployable app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b674e2",
   "metadata": {},
   "source": [
    "#### Write food101 class names to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b21b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "food101_class_names = train_data_vit.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc294374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a example directory\n",
    "foodvision_101_examples_path = Path(\"demos/foodvision_100percent/examples\")\n",
    "foodvision_101_examples_path.mkdir(parents = True,\n",
    "                                   exist_ok = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "foodvision_big_class_names_path = foodvision_big_path / \"class_names.txt\"\n",
    "\n",
    "with open(foodvision_big_class_names_path, \"w\") as f:\n",
    "    print(f\"[INFO] saving food101 class names to {foodvision_big_class_names_path}\")\n",
    "    f.write(\"\\n\".join(food101_class_names)) # new line per class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f84778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open food101 class names file and read each line into a list\n",
    "with open(foodvision_big_class_names_path, \"r\") as f:\n",
    "    food101_class_names_loaded = [food.strip('\\n') for food in f.readlines()]\n",
    "    \n",
    "food101_class_names_loaded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b0ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_100percent/requirements.txt\n",
    "\n",
    "torch>1.12.0\n",
    "torchvision>0.13.0\n",
    "gradio>3.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71dfff",
   "metadata": {},
   "source": [
    "#### model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_100percent/model.py\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def create_vit_model(num_classes:int=101, \n",
    "                          seed:int=42):\n",
    "    \"\"\"Creates an ViT feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head. \n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): ViT feature extractor model. \n",
    "        transforms (torchvision.transforms): ViT image transforms.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate model to load saved state dict()\n",
    "    \n",
    "    # Create ViT pretrained weights, transforms and model\n",
    "    vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # 'DEFAULT' = best available\n",
    "    transforms = vit_weights.transforms()\n",
    "    model = torchvision.models.vit_b_16(weights = vit_weights)\n",
    "\n",
    "    # Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head with random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    model.heads = nn.Linear(in_features = 768,\n",
    "                        out_features = 101)\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef0308",
   "metadata": {},
   "source": [
    "#### app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45bf38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_100percent/app.py\n",
    "\n",
    "# 1.\n",
    "import gradio as gr\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from model import create_vit\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Setup class names\n",
    "with open(\"class_names.txt\", \"r\") as f:\n",
    "    class_names = [food.strip('\\n') for food in f.readlines()]\n",
    "\n",
    "# 2.\n",
    "vit_food101, vit_transforms = create_vit_model(num_classes = 101)\n",
    "\n",
    "# Load saved weights\n",
    "vit_food101.load_state_dict(torch.load(\"models/state_dict__vit_food101_100_percent.pth\",\n",
    "                                           map_location = torch.device('cpu')))\n",
    "\n",
    "\n",
    "# 3.\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    \n",
    "    # Start a timer\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Transform the input image for use with vit\n",
    "    img = vit_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    # Put the model into eval mode, make prediction\n",
    "    vit_food101.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass transformed image through the model and turn the prediction logits into probabilities\n",
    "        pred_probs = torch.softmax(vit_food101(img), dim = 1)\n",
    "    \n",
    "    # Create a prediction label and prediction probability dictionary\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    # Calculate pre time\n",
    "    end_time = timer()\n",
    "    pred_time = round(end_time - start_time, 4)\n",
    "    \n",
    "    # Return pred dict and pred time\n",
    "    return pred_labels_and_probs, pred_time\n",
    "\n",
    "# 4.\n",
    "title = 'FoodIdentifier Big 💪🍕'\n",
    "description = \"A Vision Transformer feature extractor computer vision model to classify images as pizza, sushi or steak\"\n",
    "article = \" anything I want for the description of the description above 🤪\"\n",
    "\n",
    "# Create example list\n",
    "# Get example filepaths in a list of lists\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "# Create the Gradio demo\n",
    "demo = gr.Interface(fn = predict, # maps input to output\n",
    "                    inputs = gr.Image(type = 'pil'),\n",
    "                    outputs = [gr.Label(num_top_classes = 5, label = \"Predictions\"),\n",
    "                              gr.Number(label = \"Prediction time (s)\")],\n",
    "                    examples = example_list,\n",
    "                    title = title,\n",
    "                    description = description,\n",
    "                    article = article\n",
    "                   )\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch(debug = False, # print errors locally?\n",
    "           share = True) # generate a publically shareable URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc1457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
